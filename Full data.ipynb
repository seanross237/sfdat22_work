{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12091, 16)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('~/desktop/zfirst_week_with_tms_less_callsigns3.csv')\n",
    "\n",
    "data2 = pd.read_csv('~/desktop/second_week.csv')\n",
    "\n",
    "\n",
    "data= data[(data.watchers > 10) & (data.show != 'u') & (data.show != '2016 NCAA Basketball Tournament')]\n",
    "data2= data2[(data2.watchers > 10) & (data2.show != 'u') & (data2.show != '2016 Masters Tournament')]\n",
    "\n",
    "data= data.reset_index(drop=True)\n",
    "data2=data2.reset_index(drop=True)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11439, 16)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 90)\n",
    "callsign_dummies = pd.get_dummies(data.callsign)\n",
    "content_type_dummies = pd.get_dummies(data.content_type)\n",
    "daypart_dummies = pd.get_dummies(data.daypart)\n",
    "show_dummies = pd.get_dummies(data.show)\n",
    "\n",
    "\n",
    "callsign_dummies2 = pd.get_dummies(data2.callsign)\n",
    "content_type_dummies2 = pd.get_dummies(data2.content_type)\n",
    "daypart_dummies2 = pd.get_dummies(data2.daypart)\n",
    "show_dummies2 = pd.get_dummies(data2.show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23530, 16)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data = pd.concat([data,data2])\n",
    "big_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#JUST WEEK 1\n",
    "feature_df = pd.concat([content_type_dummies, callsign_dummies,daypart_dummies, data[['weekend']]], axis=1)\n",
    "feature_df2 =pd.concat([content_type_dummies2, callsign_dummies2,daypart_dummies2, data2[['weekend']]], axis=1)\n",
    "\n",
    "feature_with_show=pd.concat([content_type_dummies, callsign_dummies,daypart_dummies, show_dummies, data[['weekend']]], axis=1)\n",
    "feature_with_show2=pd.concat([content_type_dummies2, callsign_dummies2,daypart_dummies2, show_dummies2, data2[['weekend']]], axis=1)\n",
    "\n",
    "target_df = data.watchers\n",
    "target_dfanother=data2.watchers\n",
    "\n",
    "\n",
    "target_df2 = data[['watchers']]\n",
    "target_dfanother2 = data2[['watchers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23530, 148)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#JUST WEEK 2\n",
    "big_feature_df = pd.concat([feature_df,feature_df2])\n",
    "\n",
    "big_target_df = pd.concat([target_df2,target_dfanother2])\n",
    "\n",
    "big_feature_df =big_feature_df.fillna(0)\n",
    "\n",
    "big_show_feature_df=pd.concat([feature_with_show,feature_with_show2])\n",
    "big_show_feature_df =big_show_feature_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23530, 2556)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_show_feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['cid'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-415-27469051ff5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeature_columns_with_extra\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_columns_with_extra\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeature_columns_with_extra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeature_columns_with_extra\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeature_columns_with_extra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'show'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeature_columns_with_extra\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeature_columns_with_extra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tms10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeature_columns_with_extra\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeature_columns_with_extra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   1595\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   2568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2570\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels %s not contained in axis'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2571\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['cid'] not contained in axis"
     ]
    }
   ],
   "source": [
    "#ignore this \n",
    "\n",
    "feature_columns_with_extra =feature_df.drop('watchers', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('cid', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('show', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('tms10', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('description', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('air_time', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('daypart', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('genre1', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('genre2', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('genre3', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('actor1', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('actor2', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('actor3', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('callsign', axis=1)\n",
    "feature_columns_with_extra =feature_columns_with_extra.drop('content_type', axis=1)\n",
    "\n",
    "feature_columns = feature_columns_with_extra\n",
    "\n",
    "watchers_column = dope_df[['watchers']]\n",
    "\n",
    "\n",
    "#feature_cols = [feature_columns]\n",
    "#feature_cols = [cols1]\n",
    "#X = result[feature_cols]\n",
    "#y = result.watchers\n",
    "\n",
    "\n",
    "#logreg = LogisticRegression(C=1e9)\n",
    "\n",
    "#logreg.fit(feature_columns, watchers_column)\n",
    "#assorted_pred_class = logreg.predict(feature_columns)\n",
    "\n",
    "#assorted_pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -123.777776856\n",
      "200 -122.606073828\n",
      "200 -121.462712829\n",
      "200 -120.918856775\n",
      "200 -120.328051472\n",
      "200 -119.987373025\n",
      "200 -119.279319861\n",
      "200 -118.444881993\n",
      "200 -117.767330772\n",
      "200 -116.874282156\n",
      "200 -116.301827264\n",
      "200 -115.586131529\n",
      "200 -114.788133055\n"
     ]
    }
   ],
   "source": [
    "#attempting decision tree\n",
    "\n",
    "max_depth_range = range(24, 37)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "MAE_scores =[]\n",
    "# use 10-fold cross-validation with each value of max_depth\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth)\n",
    "    MSE_score = (np.mean(cross_val_score(treereg, big_show_feature_df, big_target_df, cv=10, scoring='mean_absolute_error')))\n",
    "    print estimator, MSE_score\n",
    "    #RMSE_scores.append(np.mean(np.sqrt(-MSE_score)))\n",
    "    #print depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-92.1026209889\n"
     ]
    }
   ],
   "source": [
    "#attempting decision tree BIGGER DATA\n",
    "\n",
    "X_important = treereg.transform(big_feature_with_genre_df, threshold='mean')\n",
    "\n",
    "\n",
    "max_depth_range = range(70, 72, 5)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "MAE_scores =[]\n",
    "# use 10-fold cross-validation with each value of max_depth\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth)\n",
    "    treereg.fit(big_feature_with_genre_df, big_target_df)\n",
    "    #MAE_score = (np.mean(cross_val_score(treereg, big_feature_with_genre_df, big_target_df, cv=10, scoring='mean_absolute_error')))\n",
    "    MAE_scores.append(-MAE_score)\n",
    "    print MAE_score\n",
    "    #RMSE_scores.append(np.mean(np.sqrt(-MSE_score)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_cols = [h for h in big_feature_with_genre_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACMAXHD</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Motorcycle Racing</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Martial Arts</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Latin</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Poker</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Pop</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Kayaking</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>HDPPV</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>R&amp;b</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MAXHD</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Running</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>MTVLIVE</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Fencing</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Snowmobile</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Environment</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Event</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Musical Comedy</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Alternative</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Bowling</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BETJ</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>PPV1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Diving</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Computers</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Curling</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Concert</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Swimming</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Romantic Comedy</td>\n",
       "      <td>3.317852e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>War</td>\n",
       "      <td>1.784267e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Military</td>\n",
       "      <td>3.251495e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>earlyfringe</td>\n",
       "      <td>8.033334e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BRAVOHD</td>\n",
       "      <td>8.275729e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>8.457489e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Awards</td>\n",
       "      <td>8.505482e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>earlymorning</td>\n",
       "      <td>8.649633e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Sitcom</td>\n",
       "      <td>8.884317e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Talk</td>\n",
       "      <td>1.000291e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Horror</td>\n",
       "      <td>1.020764e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Reality</td>\n",
       "      <td>1.033210e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>FNCHD</td>\n",
       "      <td>1.056728e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ESPNHD</td>\n",
       "      <td>1.066416e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>overnight</td>\n",
       "      <td>1.092545e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>daytime</td>\n",
       "      <td>1.140564e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SH</td>\n",
       "      <td>1.149178e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Medical</td>\n",
       "      <td>1.160167e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Game Show</td>\n",
       "      <td>1.284078e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>latefringe</td>\n",
       "      <td>1.332339e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ESPN2HD</td>\n",
       "      <td>1.742388e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>EP</td>\n",
       "      <td>1.879312e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>HGTVD</td>\n",
       "      <td>1.885647e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Drama</td>\n",
       "      <td>2.301211e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Music</td>\n",
       "      <td>2.355173e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMCHD</td>\n",
       "      <td>2.975792e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>WNYWDT</td>\n",
       "      <td>3.556191e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>WNBCDT</td>\n",
       "      <td>5.632073e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>primetime</td>\n",
       "      <td>6.345940e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>WABCDT</td>\n",
       "      <td>6.512324e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>weekend</td>\n",
       "      <td>6.833715e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>WCBSDT</td>\n",
       "      <td>7.758509e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>News</td>\n",
       "      <td>9.450584e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature    importance\n",
       "0              ACMAXHD  0.000000e+00\n",
       "228  Motorcycle Racing  0.000000e+00\n",
       "224       Martial Arts  0.000000e+00\n",
       "222              Latin  0.000000e+00\n",
       "242              Poker  0.000000e+00\n",
       "244                Pop  0.000000e+00\n",
       "221           Kayaking  0.000000e+00\n",
       "63               HDPPV  0.000000e+00\n",
       "247                R&b  0.000000e+00\n",
       "148                NaN  0.000000e+00\n",
       "74               MAXHD  0.000000e+00\n",
       "253            Running  0.000000e+00\n",
       "81             MTVLIVE  0.000000e+00\n",
       "199            Fencing  0.000000e+00\n",
       "260         Snowmobile  0.000000e+00\n",
       "193        Environment  0.000000e+00\n",
       "195              Event  0.000000e+00\n",
       "232     Musical Comedy  0.000000e+00\n",
       "152        Alternative  0.000000e+00\n",
       "166            Bowling  0.000000e+00\n",
       "6                 BETJ  0.000000e+00\n",
       "92                PPV1  0.000000e+00\n",
       "186             Diving  0.000000e+00\n",
       "176          Computers  0.000000e+00\n",
       "183            Curling  0.000000e+00\n",
       "177            Concert  0.000000e+00\n",
       "266           Swimming  0.000000e+00\n",
       "252    Romantic Comedy  3.317852e-10\n",
       "274                War  1.784267e-09\n",
       "226           Military  3.251495e-09\n",
       "..                 ...           ...\n",
       "141        earlyfringe  8.033334e-03\n",
       "10             BRAVOHD  8.275729e-03\n",
       "173             Comedy  8.457489e-03\n",
       "161             Awards  8.505482e-03\n",
       "142       earlymorning  8.649633e-03\n",
       "258             Sitcom  8.884317e-03\n",
       "267               Talk  1.000291e-02\n",
       "215             Horror  1.020764e-02\n",
       "248            Reality  1.033210e-02\n",
       "41               FNCHD  1.056728e-02\n",
       "36              ESPNHD  1.066416e-02\n",
       "144          overnight  1.092545e-02\n",
       "140            daytime  1.140564e-02\n",
       "96                  SH  1.149178e-02\n",
       "225            Medical  1.160167e-02\n",
       "203          Game Show  1.284078e-02\n",
       "143         latefringe  1.332339e-02\n",
       "33             ESPN2HD  1.742388e-02\n",
       "32                  EP  1.879312e-02\n",
       "64               HGTVD  1.885647e-02\n",
       "190              Drama  2.301211e-02\n",
       "230              Music  2.355173e-02\n",
       "2                AMCHD  2.975792e-02\n",
       "136             WNYWDT  3.556191e-02\n",
       "133             WNBCDT  5.632073e-02\n",
       "146          primetime  6.345940e-02\n",
       "128             WABCDT  6.512324e-02\n",
       "147            weekend  6.833715e-02\n",
       "129             WCBSDT  7.758509e-02\n",
       "235               News  9.450584e-02\n",
       "\n",
       "[277 rows x 2 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_}).sort('importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23530"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23530"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194.08402039949002"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_y = []\n",
    "y_pred = big_target_df[['watchers']]\n",
    "for x in range(0,len(big_target_df)):\n",
    "    null_y.append(205)\n",
    "\n",
    "\n",
    "metrics.mean_absolute_error(null_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(null_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a1e64b50>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAESCAYAAADjS5I+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8DpSUVwZrCYpUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b34fab90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(max_depth_range, RMSE_scores)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('MAE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11439, 143)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  converted to the first listed format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123.69857886602338"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attempting bagged decision\n",
    "#samples = [np.random.choice(a=len(data), size=len(data), replace=True) for _ in range(1, 11)]\n",
    "target_df2 = data[['watchers']]\n",
    "\n",
    "#treereg = DecisionTreeRegressor(max_depth=None)\n",
    "X_train = big_feature_with_genre_df.iloc[:18000,:]\n",
    "Y_train = big_target_df.iloc[:18000:,:]\n",
    "\n",
    "X_test = big_feature_with_genre_df.iloc[18000:,:]\n",
    "y_test = big_target_df.iloc[18000:,:]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, max_features=143, bootstrap=True, oob_score=True)\n",
    "\n",
    "\n",
    "bagreg.fit(X_train, Y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "\n",
    "# calculate RMSE\n",
    "metrics.mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "#bagreg.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  converted to the first listed format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101.79541572245797"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attempting bagged decision BIGGER DATA\n",
    "#samples = [np.random.choice(a=len(data), size=len(data), replace=True) for _ in range(1, 11)]\n",
    "target_df2 = data[['watchers']]\n",
    "\n",
    "#treereg = DecisionTreeRegressor(max_depth=None)\n",
    "X_train = big_feature_with_genre_df.iloc[:18000,:]\n",
    "Y_train = big_target_df.iloc[:18000:,:]\n",
    "\n",
    "X_test = big_feature_with_genre_df.iloc[18000:,:]\n",
    "y_test = big_target_df.iloc[18000:,:]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, max_features=277, bootstrap=True, oob_score=True)\n",
    "\n",
    "\n",
    "bagreg.fit(X_train, Y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "\n",
    "# calculate RMSE\n",
    "\n",
    "metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23530, 277)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_feature_with_genre_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.53743218806511"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.mean_absolute_error(y_test, y_pred)\n",
    "null_acc = []\n",
    "for x in range(0,len(y_test)):\n",
    "    null_acc.append(205)\n",
    "    \n",
    "metrics.mean_absolute_error(y_test, null_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekend</th>\n",
       "      <th>nan</th>\n",
       "      <th>Action</th>\n",
       "      <th>Action Sports</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Alternative</th>\n",
       "      <th>Animals</th>\n",
       "      <th>Anthology</th>\n",
       "      <th>Art</th>\n",
       "      <th>Arts/crafts</th>\n",
       "      <th>Auction</th>\n",
       "      <th>Auto</th>\n",
       "      <th>Auto Racing</th>\n",
       "      <th>Aviation</th>\n",
       "      <th>Awards</th>\n",
       "      <th>Baseball</th>\n",
       "      <th>Basketball</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Bowling</th>\n",
       "      <th>Boxing</th>\n",
       "      <th>Bull Riding</th>\n",
       "      <th>Bus./financial</th>\n",
       "      <th>Card Games</th>\n",
       "      <th>Cheerleading</th>\n",
       "      <th>Collectibles</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Comedy Drama</th>\n",
       "      <th>Community</th>\n",
       "      <th>Computers</th>\n",
       "      <th>Concert</th>\n",
       "      <th>Consumer</th>\n",
       "      <th>Cooking</th>\n",
       "      <th>Country</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Crime Drama</th>\n",
       "      <th>Curling</th>\n",
       "      <th>Dance</th>\n",
       "      <th>Dark Comedy</th>\n",
       "      <th>Diving</th>\n",
       "      <th>Docudrama</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drag Racing</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Educational</th>\n",
       "      <th>Entertainment</th>\n",
       "      <th>...</th>\n",
       "      <th>SHOWXHD</th>\n",
       "      <th>SMTHHD</th>\n",
       "      <th>SNYHD</th>\n",
       "      <th>SPIKEHD</th>\n",
       "      <th>SPROUTH</th>\n",
       "      <th>STZEHD</th>\n",
       "      <th>STZENHD</th>\n",
       "      <th>STZENWS</th>\n",
       "      <th>STZHD</th>\n",
       "      <th>SUNDHD</th>\n",
       "      <th>SYFYHD</th>\n",
       "      <th>TBSHD</th>\n",
       "      <th>TCMHD</th>\n",
       "      <th>TLCHD</th>\n",
       "      <th>TNCK</th>\n",
       "      <th>TNTHD</th>\n",
       "      <th>TOONHD</th>\n",
       "      <th>TRAVHD</th>\n",
       "      <th>TRUTVHD</th>\n",
       "      <th>TVLNDHD</th>\n",
       "      <th>TVONE</th>\n",
       "      <th>UHD</th>\n",
       "      <th>USAHD</th>\n",
       "      <th>VEL</th>\n",
       "      <th>VH1HD</th>\n",
       "      <th>VICEHD</th>\n",
       "      <th>WABCDT</th>\n",
       "      <th>WCBSDT</th>\n",
       "      <th>WE</th>\n",
       "      <th>WEATHHD</th>\n",
       "      <th>WEHD</th>\n",
       "      <th>WNBCDT</th>\n",
       "      <th>WNETDT</th>\n",
       "      <th>WNJUDT</th>\n",
       "      <th>WNYWDT</th>\n",
       "      <th>WPIXDT</th>\n",
       "      <th>WPXNDT</th>\n",
       "      <th>WXTVDT</th>\n",
       "      <th>daytime</th>\n",
       "      <th>earlyfringe</th>\n",
       "      <th>earlymorning</th>\n",
       "      <th>latefringe</th>\n",
       "      <th>overnight</th>\n",
       "      <th>primeaccess</th>\n",
       "      <th>primetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   weekend  NaN  Action  Action Sports  Adventure  Alternative  Animals  \\\n",
       "0        0    0       0              0          0            0        0   \n",
       "\n",
       "   Anthology  Art  Arts/crafts  Auction  Auto  Auto Racing  Aviation  Awards  \\\n",
       "0          0    0            0        0     0            0         0       0   \n",
       "\n",
       "   Baseball  Basketball  Biography  Bowling  Boxing  Bull Riding  \\\n",
       "0         0           0          0        0       0            0   \n",
       "\n",
       "   Bus./financial  Card Games  Cheerleading  Collectibles  Comedy  \\\n",
       "0               0           0             0             0       0   \n",
       "\n",
       "   Comedy Drama  Community  Computers  Concert  Consumer  Cooking  Country  \\\n",
       "0             0          0          0        0         0        0        0   \n",
       "\n",
       "   Crime  Crime Drama  Curling  Dance  Dark Comedy  Diving  Docudrama  \\\n",
       "0      0            0        0      0            0       0          0   \n",
       "\n",
       "   Documentary  Drag Racing  Drama  Educational  Entertainment    ...      \\\n",
       "0            0            0      0            0              0    ...       \n",
       "\n",
       "   SHOWXHD  SMTHHD  SNYHD  SPIKEHD  SPROUTH  STZEHD  STZENHD  STZENWS  STZHD  \\\n",
       "0        0       0      0        0        0       0        0        0      0   \n",
       "\n",
       "   SUNDHD  SYFYHD  TBSHD  TCMHD  TLCHD  TNCK  TNTHD  TOONHD  TRAVHD  TRUTVHD  \\\n",
       "0       0       0      0      0      0     0      0       0       0        0   \n",
       "\n",
       "   TVLNDHD  TVONE  UHD  USAHD  VEL  VH1HD  VICEHD  WABCDT  WCBSDT  WE  \\\n",
       "0        0      0    0      0    0      0       0       0       0   0   \n",
       "\n",
       "   WEATHHD  WEHD  WNBCDT  WNETDT  WNJUDT  WNYWDT  WPIXDT  WPXNDT  WXTVDT  \\\n",
       "0        0     0       0       0       0       0       0       0       0   \n",
       "\n",
       "   daytime  earlyfringe  earlymorning  latefringe  overnight  primeaccess  \\\n",
       "0        0            0             0           1          0            0   \n",
       "\n",
       "   primetime  \n",
       "0          0  \n",
       "\n",
       "[1 rows x 270 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train2 = watchers_column.watchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4299767831053678"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagreg.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg\n",
    "\n",
    "estimator_range = range(390, 400, 10)\n",
    "\n",
    "RMSE_scores=[]\n",
    "\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=estimator, max_features=277)\n",
    "    MAE_scores = cross_val_score(rfreg, feature_df, target_df, cv=5, scoring='mean_absolute_error')\n",
    "    #RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "    print estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-104.16442648289417"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(MAE_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384.92542474629158,\n",
       " 376.0318488910869,\n",
       " 376.27861649072588,\n",
       " 373.31736825868614,\n",
       " 367.05568278790349,\n",
       " 364.82315260526065,\n",
       " 367.43823022628101,\n",
       " 361.19469392998315,\n",
       " 359.217717914525,\n",
       " 360.73007009375067,\n",
       " 360.99441186603616,\n",
       " 361.05862422505771,\n",
       " 368.35611464725247,\n",
       " 355.80557083367614,\n",
       " 351.87932391732113,\n",
       " 351.00075434998223,\n",
       " 351.87932391732113,\n",
       " 351.00075434998223,\n",
       " 351.87932391732113,\n",
       " 351.00075434998223]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.plot(estimator_range, RMSE_scores)\n",
    "#plt.xlabel('n_estimators')\n",
    "#plt.ylabel('RMSE (lower is better)')\n",
    "RMSE_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "grreg = GradientBoostingRegressor()\n",
    "grreg\n",
    "\n",
    "estimator_range = range(310, 360, 50)\n",
    "\n",
    "MAE_scores=[]\n",
    "\n",
    "for estimator in estimator_range:\n",
    "    grreg = GradientBoostingRegressor(n_estimators=estimator, max_depth=50)\n",
    "    wurk = cross_val_score(grreg, big_feature_with_genre_df, big_target_df.watchers, cv=5, scoring='mean_absolute_error')\n",
    "    MAE_scores.append(np.mean((-wurk)))\n",
    "    print estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[107.17787921926319]"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bayes leggo\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Xx_train, Xx_test, yy_train, yy_test = train_test_split(big_feature_with_genre_df, big_target_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.91568927417984"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(fit_prior=True)\n",
    "nb.fit(Xx_train, yy_train.watchers)\n",
    "preds = nb.predict(Xx_test)\n",
    "metrics.mean_absolute_error(yy_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.45559440559441"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_absolute_error(yy_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205.800419617\n"
     ]
    }
   ],
   "source": [
    "print data.watchers.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xx_train, yy_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.30910966981133"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "Xx_train, Xx_test, yy_train, yy_test = train_test_split(big_feature_df, big_target_df)\n",
    "\n",
    "\n",
    "X = big_feature_df\n",
    "y = big_target_df\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(Xx_train, yy_train)\n",
    "\n",
    "y_pred = linreg.predict(Xx_test)\n",
    "#metrics.r2_score(y, y_pred)\n",
    "metrics.mean_absolute_error(yy_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weekend', 11.950311468792904),\n",
       " (nan, -190836107353266.56),\n",
       " ('Action', -11.039111170769417),\n",
       " ('Action Sports', -0.29392598185867413),\n",
       " ('Adventure', 56.948097292025537),\n",
       " ('Alternative', -90.848677873013827),\n",
       " ('Animals', -78.312411926900367),\n",
       " ('Anthology', 152.05768551365097),\n",
       " ('Art', -120.49221419932057),\n",
       " ('Arts/crafts', 97.057865233320911),\n",
       " ('Auction', -4.0100610941624719),\n",
       " ('Auto', -56.354519059911212),\n",
       " ('Auto Racing', 259.1084793850805),\n",
       " ('Aviation', 14.464417780808219),\n",
       " ('Awards', 1785.9529334724475),\n",
       " ('Baseball', 20.200681730775216),\n",
       " ('Basketball', 333.07210565861817),\n",
       " ('Biography', -35.124935308218539),\n",
       " ('Bowling', -335.81481057253586),\n",
       " ('Boxing', -0.64491412233844869),\n",
       " ('Bull Riding', -115.12593580961928),\n",
       " ('Bus./financial', -43.284757788893266),\n",
       " ('Card Games', 111.54119288727166),\n",
       " ('Cheerleading', -211.81325352869166),\n",
       " ('Collectibles', -2.6109502002114766),\n",
       " ('Comedy', -19.311239442628519),\n",
       " ('Comedy Drama', 75.373903776903902),\n",
       " ('Community', -263.71221914497949),\n",
       " ('Computers', 85.340303173986129),\n",
       " ('Concert', -228.0697035981506),\n",
       " ('Consumer', 174.45126694132199),\n",
       " ('Cooking', 26.958803086192489),\n",
       " ('Country', 496.71931778168266),\n",
       " ('Crime', 36.591577994340611),\n",
       " ('Crime Drama', 90.35280863536461),\n",
       " ('Curling', 101.10771662859851),\n",
       " ('Dance', 336.25404416371259),\n",
       " ('Dark Comedy', -162.92347678159584),\n",
       " ('Diving', -3808324393323640.0),\n",
       " ('Docudrama', -90.24429835981303),\n",
       " ('Documentary', -35.74232030252881),\n",
       " ('Drag Racing', 150.48994388586434),\n",
       " ('Drama', 218.62940226294822),\n",
       " ('Educational', 6.9271897397926665),\n",
       " ('Entertainment', -73.181115531736538),\n",
       " ('Environment', -400.85336848206919),\n",
       " ('Erotic', -46.246523390724199),\n",
       " ('Event', -521.82625467798607),\n",
       " ('Exercise', -4.0400689883525871),\n",
       " ('Fantasy', 1.6229837145892816),\n",
       " ('Fashion', -98.770272681423677),\n",
       " ('Fencing', -90.507006021987422),\n",
       " ('Figure Skating', -33.416260128000189),\n",
       " ('Fishing', 17.460663930453784),\n",
       " ('Football', -19.146986121255679),\n",
       " ('Game Show', -0.47076464824093023),\n",
       " ('Gaming', 8.5938893608013025),\n",
       " ('Gay/lesbian', 9.083837554048813),\n",
       " ('Golf', 882.00242222747715),\n",
       " ('Gospel', -543.82945021871819),\n",
       " ('Gymnastics', 35.20488284011914),\n",
       " ('Health', 45.196928195160481),\n",
       " ('Hip-hop & Rap', -285.19667981124093),\n",
       " ('Historical Drama', 32.99704012342751),\n",
       " ('History', 11.78708444156306),\n",
       " ('Hockey', -271.77530413920408),\n",
       " ('Home Improvement', 32.49515503636151),\n",
       " ('Horror', 6.8190780032085314),\n",
       " ('House/garden', -6.8272759634472067),\n",
       " ('How-to', -62.798600546789665),\n",
       " ('Hunting', -52.944299122633865),\n",
       " ('Interview', -64.381772504834188),\n",
       " ('Intl Soccer', 184.58732331128169),\n",
       " ('Kayaking', -35.522450719984711),\n",
       " ('Latin', -438.715417551602),\n",
       " ('Law', -62.818507655166279),\n",
       " ('Martial Arts', 12.323471909909259),\n",
       " ('Medical', -7.6142150559851771),\n",
       " ('Military', 61.217140006951539),\n",
       " ('Mixed Martial Arts', -61.403435781201274),\n",
       " ('Motorcycle Racing', 101.75731294688005),\n",
       " ('Motorsports', 95.536385180783924),\n",
       " ('Music', 284.13958073782197),\n",
       " ('Musical', 185.09186699360828),\n",
       " ('Musical Comedy', 124.11226471477008),\n",
       " ('Mystery', -101.07192712266448),\n",
       " ('Nature', -27.394913232492719),\n",
       " ('News', 200.32024295912797),\n",
       " ('Newsmagazine', -123.88952376267986),\n",
       " ('Outdoors', 4.5171766649500587),\n",
       " ('Paranormal', -15.51805080666108),\n",
       " ('Parenting', 183.13740032858595),\n",
       " ('Performing Arts', 796.19036695126965),\n",
       " ('Pets', -19.888258024052305),\n",
       " ('Poker', -151.91996513086391),\n",
       " ('Politics', 146.3772061682584),\n",
       " ('Pop', -207.76111035918555),\n",
       " ('Pro Wrestling', 713.39230007671631),\n",
       " ('Public Affairs', -120.18920417514201),\n",
       " ('R&b', -273.15456813000355),\n",
       " ('Reality', 46.924493276292324),\n",
       " ('Religious', 45.058040483342744),\n",
       " ('Rock', -70.647295684273786),\n",
       " ('Romance', -109.2597357540137),\n",
       " ('Romantic Comedy', -319.86265287093704),\n",
       " ('Running', 112.62294639894208),\n",
       " ('Science', 0.088699448058079611),\n",
       " ('Science Fiction', 39.500665686567281),\n",
       " ('Self Improvement', -31.088551156385563),\n",
       " ('Shopping', 194.95290755456392),\n",
       " ('Sitcom', -30.137005215746669),\n",
       " ('Snowmobile', 38.017470633168521),\n",
       " ('Soap', 188.05335682548812),\n",
       " ('Soccer', 83.20060265777775),\n",
       " ('Sports Related', -22.594781590978215),\n",
       " ('Sports Talk', 242.35968600499132),\n",
       " ('Standup', 59.548821122098595),\n",
       " ('Swimming', 3808324393323676.0),\n",
       " ('Talk', 76.134287994962577),\n",
       " ('Technology', 3.0622497693350113),\n",
       " ('Tennis', -448.16350867575068),\n",
       " ('Theater', -289.36779991814308),\n",
       " ('Thriller', 61.933169425492693),\n",
       " ('Travel', 6.1087164240021359),\n",
       " ('Variety', 106.39205892233116),\n",
       " ('War', -83.39252191680049),\n",
       " ('Weather', 66.316882676004838),\n",
       " ('Western', -178.11592481628577),\n",
       " ('EP', -767751386366939.25),\n",
       " ('SH', -767751386366964.62),\n",
       " ('ACMAXHD', -1366596655635631.8),\n",
       " ('AETVHD', -1366596655635570.0),\n",
       " ('AMCHD', -1366596655635212.8),\n",
       " ('APLHD', -1366596655635578.2),\n",
       " ('AXSTV', -1366596655635977.8),\n",
       " ('BETHD', -1366596655635585.5),\n",
       " ('BETSOUL', -1366596655636020.5),\n",
       " ('BIG10HD', -1366596655635771.0),\n",
       " ('BLOOMHD', -1366596655635699.5),\n",
       " ('BRAVOHD', -1366596655635440.2),\n",
       " ('CBSSNHD', -1366596655635993.2),\n",
       " ('CCHD', -1366596655635721.8),\n",
       " ('CINHD', -1366596655635820.8),\n",
       " ('CMAXHD', -1366596655635943.0),\n",
       " ('CMTVHD', -1366596655635698.8),\n",
       " ('CNBCHD', -1366596655635704.0),\n",
       " ('CNNHD', -1366596655635329.2),\n",
       " ('COOKHD', -1366596655635790.0),\n",
       " ('CSNCHD', -1366596655635717.5),\n",
       " ('CTRC', -1366596655635691.8),\n",
       " ('DESTHD', -1366596655635796.8),\n",
       " ('DISNHD', -1366596655635438.8),\n",
       " ('DIYHD', -1366596655635755.8),\n",
       " ('DJCHHD', -1366596655635676.5),\n",
       " ('DLC', -1366596655635820.0),\n",
       " ('DSCHD', -1366596655635443.8),\n",
       " ('DXDHD', -1366596655635733.2),\n",
       " ('EHD', -1366596655635610.5),\n",
       " ('ENCRHD', -1366596655636126.0),\n",
       " ('ENCRWS', -1366596655635658.8),\n",
       " ('ESPN2HD', -1366596655635132.0),\n",
       " ('ESPNCL', -1366596655635788.8),\n",
       " ('ESPND', -1366596655635955.8),\n",
       " ('ESPNHD', -1366596655635072.0),\n",
       " ('ESPNUHD', -1366596655635632.5),\n",
       " ('ESPNWHD', -1366596655635580.5),\n",
       " ('ESQHD', -1366596655635794.0),\n",
       " ('FBNHD', -1366596655635764.8),\n",
       " ('FNCHD', -1366596655635256.8),\n",
       " ('FOODHD', -1366596655635484.2),\n",
       " ('FREFMHD', -1366596655635670.8),\n",
       " ('FS1HD', -1366596655635709.0),\n",
       " ('FSNHDMW', -1366596655635763.0),\n",
       " ('FSNO1HD', -1366596655635731.8),\n",
       " ('FXDEP', -1366596655635737.0),\n",
       " ('FXHD', -1366596655635550.2),\n",
       " ('FXXHD', -1366596655635718.0),\n",
       " ('FYIHD', -1366596655635788.5),\n",
       " ('GAC', -1366596655635875.5),\n",
       " ('GALAHD', -1366596655635708.0),\n",
       " ('GOLFHD', -1366596655636298.5),\n",
       " ('GSNHD', -1366596655635779.0),\n",
       " ('HALLHD', -1366596655635726.2),\n",
       " ('HBO2HD', -1366596655635557.8),\n",
       " ('HBOCHD', -1366596655635581.5),\n",
       " ('HBOFHD', -1366596655635741.2),\n",
       " ('HBOHD', -1366596655635455.0),\n",
       " ('HBOSGHD', -1366596655635318.2),\n",
       " ('HBOZHD', -1366596655635594.0),\n",
       " ('HDPPV', -1366596655635471.2),\n",
       " ('HGTVD', -1366596655635296.2),\n",
       " ('HLNHD', -1366596655635731.0),\n",
       " ('HMMHD', -1366596655635797.2),\n",
       " ('HSTRYHD', -1366596655635515.2),\n",
       " ('IDHD', -1366596655635610.0),\n",
       " ('IFCHD', -1366596655635807.5),\n",
       " ('LIFEHD', -1366596655635643.0),\n",
       " ('LMNHD', -1366596655635640.2),\n",
       " ('LOGO', -1366596655635781.5),\n",
       " ('LRW', -1366596655635716.8),\n",
       " ('MAXHD', -1366596655635853.2),\n",
       " ('MILH', -1366596655635824.5),\n",
       " ('MLBHD', -1366596655635538.0),\n",
       " ('MNBCHD', -1366596655635741.8),\n",
       " ('MOMAXHD', -1366596655635600.2),\n",
       " ('MTV2HD', -1366596655635729.5),\n",
       " ('MTVHD', -1366596655635706.5),\n",
       " ('MTVLIVE', -1366596655636003.0),\n",
       " ('NBATV', -1366596655636130.8),\n",
       " ('NBCSNHD', -1366596655635741.8),\n",
       " ('NFLHD', -1366596655635668.0),\n",
       " ('NGCHD', -1366596655635633.0),\n",
       " ('NICJR', -1366596655635741.8),\n",
       " ('NIKHD', -1366596655635527.2),\n",
       " ('NIKTON', -1366596655635728.5),\n",
       " ('OWN', -1366596655635807.5),\n",
       " ('OXYGNHD', -1366596655635617.0),\n",
       " ('POPHD', -1366596655635714.0),\n",
       " ('PPV1', -1366596655635366.5),\n",
       " ('QVCHD', -1366596655635924.5),\n",
       " ('REELZ', -1366596655635669.8),\n",
       " ('SCIHD', -1366596655635657.0),\n",
       " ('SHO2HD', -1366596655635827.8),\n",
       " ('SHOCSHD', -1366596655635822.5),\n",
       " ('SHOWHD', -1366596655635501.8),\n",
       " ('SHOWXHD', -1366596655635743.5),\n",
       " ('SMTHHD', -1366596655635727.5),\n",
       " ('SNYHD', -1366596655635836.8),\n",
       " ('SPIKEHD', -1366596655635603.0),\n",
       " ('SPROUTH', -1366596655635825.2),\n",
       " ('STZEHD', -1366596655636019.0),\n",
       " ('STZENHD', -1366596655636128.0),\n",
       " ('STZENWS', -1366596655635644.5),\n",
       " ('STZHD', -1366596655635940.0),\n",
       " ('SUNDHD', -1366596655635850.8),\n",
       " ('SYFYHD', -1366596655635790.8),\n",
       " ('TBSHD', -1366596655635462.2),\n",
       " ('TCMHD', -1366596655635589.2),\n",
       " ('TLCHD', -1366596655635615.5),\n",
       " ('TNCK', -1366596655635721.2),\n",
       " ('TNTHD', -1366596655635553.5),\n",
       " ('TOONHD', -1366596655635602.5),\n",
       " ('TRAVHD', -1366596655635679.0),\n",
       " ('TRUTVHD', -1366596655635613.0),\n",
       " ('TVLNDHD', -1366596655635724.2),\n",
       " ('TVONE', -1366596655635733.0),\n",
       " ('UHD', -1366596655635875.5),\n",
       " ('USAHD', -1366596655635595.0),\n",
       " ('VEL', -1366596655635606.0),\n",
       " ('VH1HD', -1366596655635664.5),\n",
       " ('VICEHD', -1366596655635814.5),\n",
       " ('WABCDT', -1366596655634910.0),\n",
       " ('WCBSDT', -1366596655634953.8),\n",
       " ('WE', -1366596655635740.5),\n",
       " ('WEATHHD', -1366596655635826.2),\n",
       " ('WEHD', -1366596655635750.0),\n",
       " ('WNBCDT', -1366596655634964.0),\n",
       " ('WNETDT', -1366596655635688.0),\n",
       " ('WNJUDT', -1366596655635702.5),\n",
       " ('WNYWDT', -1366596655635317.2),\n",
       " ('WPIXDT', -1366596655635639.0),\n",
       " ('WPXNDT', -1366596655635675.8),\n",
       " ('WXTVDT', -1366596655635611.8),\n",
       " ('daytime', 300059841351901.5),\n",
       " ('earlyfringe', 300059841351957.75),\n",
       " ('earlymorning', 300059841351866.12),\n",
       " ('latefringe', 300059841351927.75),\n",
       " ('overnight', 300059841351833.0),\n",
       " ('primeaccess', 300059841351983.81),\n",
       " ('primetime', 300059841352192.38)]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Net\n",
    "from pybrain.datasets            import ClassificationDataSet\n",
    "from pybrain.utilities           import percentError\n",
    "from pybrain.tools.shortcuts     import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules   import SoftmaxLayer\n",
    "from numpy import ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'None of [[17685]] are in the [index]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-528-c77ec14ff463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     n_iter=10)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnetwurk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/mlp.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/mlp.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             log.error(\"\\n{}{}{}\\n\\n{}\\n\".format(\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/mlp.pyc\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mis_best_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mavg_train_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mavg_train_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/backend/lasagne/mlp.pyc\u001b[0m in \u001b[0;36m_train_impl\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_valid_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/backend/lasagne/mlp.pyc\u001b[0m in \u001b[0;36m_batch_impl\u001b[0;34m(self, X, y, w, processor, mode, output, shuffle)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/backend/lasagne/mlp.pyc\u001b[0m in \u001b[0;36m_iterate_data\u001b[0;34m(self, batch_size, X, y, w, shuffle)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mexcerpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcerpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcerpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcerpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcerpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sknn/backend/lasagne/mlp.pyc\u001b[0m in \u001b[0;36mcast\u001b[0;34m(array, indices)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# Support for pandas.DataFrame, requires custom indexing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1321\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot index with multidimensional key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_validate_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m                 raise KeyError(\"None of [%s] are in the [%s]\" %\n\u001b[0;32m-> 1271\u001b[0;31m                                (key, self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'None of [[17685]] are in the [index]'"
     ]
    }
   ],
   "source": [
    "from sknn.mlp import Regressor, Layer\n",
    "from Theano.tensor.signal import pool\n",
    "\n",
    "\n",
    "X_train = big_feature_df.iloc[:18000,:]\n",
    "Y_train = big_target_df.iloc[:18000:,:]\n",
    "\n",
    "X_test = big_feature_df.iloc[18000:,:]\n",
    "y_test = big_target_df.iloc[18000:,:]\n",
    "\n",
    "netwurk = Regressor(verbose=True,layers=[\n",
    "        Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Linear\")],\n",
    "    learning_rate=0.02,\n",
    "    n_iter=10)\n",
    "netwurk.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data['genres']= data[data.columns[7:10]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "#data.genres\n",
    "\n",
    "all_genres=[]\n",
    "\n",
    "data.genre1=data.genre1.str.strip()\n",
    "data.genre2=data.genre2.str.strip()\n",
    "data.genre3=data.genre3.str.strip()\n",
    "\n",
    "\n",
    "data.genre1.apply(lambda x: all_genres.append(x))\n",
    "data.genre2.apply(lambda x: all_genres.append(x))\n",
    "data.genre3.apply(lambda x: all_genres.append(x))\n",
    "\n",
    "total_genres = set(all_genres)\n",
    "\n",
    "genre_dict_list=[] \n",
    "\n",
    "for row in range(len(data)): \n",
    "    genre_dict_list.append({x:0 for x in total_genres})\n",
    "        \n",
    "for row in range(len(data)):\n",
    "    \n",
    "    for genre in genre_dict_list[row]:\n",
    "        if (data.iloc[row][7] == genre) or (data.iloc[row][8] == genre) or (data.iloc[row][9] == genre):\n",
    "            genre_dict_list[row][genre] = 1 \n",
    "            if row % 100 == 0:\n",
    "                print row\n",
    "\n",
    "\n",
    "\n",
    "#run this 1\n",
    "all_actors=[]\n",
    "\n",
    "\n",
    "data.actor1=data.actor1.str.strip()\n",
    "data.actor2=data.actor2.str.strip()\n",
    "data.actor3=data.actor3.str.strip()\n",
    "\n",
    "data.actor1.apply(lambda x: all_actors.append(x))\n",
    "data.actor2.apply(lambda x: all_actors.append(x))\n",
    "data.actor3.apply(lambda x: all_actors.append(x))\n",
    "\n",
    "unique_actors = set(all_actors)\n",
    "\n",
    "actors_dict_list=[]\n",
    "print 'here'\n",
    "for row in range(len(data)): \n",
    "    actors_dict_list.append({x:0 for x in unique_actors})\n",
    "    \n",
    "print 'yeah'\n",
    "for row in range(len(data)):\n",
    "    for actor in actors_dict_list[row]:\n",
    "        if (data.iloc[row][10] == actor) or (data.iloc[row][11] == actor) or (data.iloc[row][12] == actor):\n",
    "            actors_dict_list[row][actor] = 1 \n",
    "            \n",
    "    if row % 100 == 0:\n",
    "        print row           \n",
    "genre_df = pd.DataFrame.from_records(genre_dict_list)\n",
    "actor_df = pd.DataFrame.from_records(actor_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "blah\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "60\n",
      "80\n",
      "80\n",
      "80\n",
      "100\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "160\n",
      "180\n",
      "200\n",
      "240\n",
      "240\n",
      "260\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "320\n",
      "320\n",
      "340\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "400\n",
      "420\n",
      "420\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "520\n",
      "540\n",
      "560\n",
      "560\n",
      "580\n",
      "580\n",
      "600\n",
      "620\n",
      "620\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "680\n",
      "680\n",
      "700\n",
      "720\n",
      "720\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "780\n",
      "800\n",
      "800\n",
      "800\n",
      "820\n",
      "840\n",
      "840\n",
      "860\n",
      "860\n",
      "880\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1020\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1080\n",
      "1080\n",
      "1100\n",
      "1100\n",
      "1120\n",
      "1120\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1160\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1200\n",
      "1200\n",
      "1220\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1260\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1300\n",
      "1300\n",
      "1320\n",
      "1320\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1360\n",
      "1360\n",
      "1380\n",
      "1380\n",
      "1400\n",
      "1400\n",
      "1400\n",
      "1420\n",
      "1420\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1540\n",
      "1540\n",
      "1560\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1660\n",
      "1660\n",
      "1680\n",
      "1680\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1720\n",
      "1740\n",
      "1740\n",
      "1760\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1800\n",
      "1820\n",
      "1820\n",
      "1840\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2080\n",
      "2100\n",
      "2100\n",
      "2100\n",
      "2120\n",
      "2120\n",
      "2140\n",
      "2140\n",
      "2160\n",
      "2160\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2240\n",
      "2260\n",
      "2260\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2320\n",
      "2320\n",
      "2340\n",
      "2340\n",
      "2360\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "2740\n",
      "2760\n",
      "2780\n",
      "2800\n",
      "2820\n",
      "2840\n",
      "2840\n",
      "2860\n",
      "2880\n",
      "2880\n",
      "2880\n",
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "2980\n",
      "2980\n",
      "3000\n",
      "3000\n",
      "3020\n",
      "3040\n",
      "3040\n",
      "3060\n",
      "3080\n",
      "3100\n",
      "3120\n",
      "3140\n",
      "3160\n",
      "3180\n",
      "3180\n",
      "3180\n",
      "3200\n",
      "3220\n",
      "3240\n",
      "3260\n",
      "3280\n",
      "3280\n",
      "3280\n",
      "3300\n",
      "3320\n",
      "3340\n",
      "3340\n",
      "3340\n",
      "3360\n",
      "3380\n",
      "3400\n",
      "3400\n",
      "3420\n",
      "3440\n",
      "3440\n",
      "3440\n",
      "3460\n",
      "3460\n",
      "3480\n",
      "3500\n",
      "3500\n",
      "3520\n",
      "3540\n",
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "3600\n",
      "3620\n",
      "3640\n",
      "3660\n",
      "3680\n",
      "3680\n",
      "3700\n",
      "3720\n",
      "3720\n",
      "3720\n",
      "3740\n",
      "3760\n",
      "3760\n",
      "3780\n",
      "3780\n",
      "3800\n",
      "3820\n",
      "3840\n",
      "3840\n",
      "3860\n",
      "3880\n",
      "3900\n",
      "3920\n",
      "3940\n",
      "3960\n",
      "3960\n",
      "3960\n",
      "3980\n",
      "3980\n",
      "4000\n",
      "4020\n",
      "4040\n",
      "4060\n",
      "4080\n",
      "4080\n",
      "4080\n",
      "4100\n",
      "4120\n",
      "4120\n",
      "4140\n",
      "4140\n",
      "4160\n",
      "4180\n",
      "4180\n",
      "4180\n",
      "4200\n",
      "4200\n",
      "4200\n",
      "4220\n",
      "4220\n",
      "4240\n",
      "4260\n",
      "4280\n",
      "4300\n",
      "4320\n",
      "4340\n",
      "4340\n",
      "4340\n",
      "4360\n",
      "4380\n",
      "4380\n",
      "4380\n",
      "4400\n",
      "4420\n",
      "4420\n",
      "4440\n",
      "4460\n",
      "4480\n",
      "4480\n",
      "4500\n",
      "4500\n",
      "4500\n",
      "4520\n",
      "4540\n",
      "4540\n",
      "4540\n",
      "4560\n",
      "4580\n",
      "4580\n",
      "4580\n",
      "4600\n",
      "4620\n",
      "4620\n",
      "4620\n",
      "4640\n",
      "4660\n",
      "4680\n",
      "4700\n",
      "4700\n",
      "4720\n",
      "4740\n",
      "4740\n",
      "4740\n",
      "4760\n",
      "4780\n",
      "4780\n",
      "4780\n",
      "4800\n",
      "4800\n",
      "4820\n",
      "4820\n",
      "4820\n",
      "4840\n",
      "4860\n",
      "4880\n",
      "4880\n",
      "4900\n",
      "4920\n",
      "4940\n",
      "4940\n",
      "4960\n",
      "4980\n",
      "5000\n",
      "5020\n",
      "5020\n",
      "5040\n",
      "5060\n",
      "5080\n",
      "5100\n",
      "5120\n",
      "5120\n",
      "5140\n",
      "5160\n",
      "5160\n",
      "5160\n",
      "5180\n",
      "5200\n",
      "5220\n",
      "5240\n",
      "5240\n",
      "5260\n",
      "5260\n",
      "5260\n",
      "5280\n",
      "5300\n",
      "5320\n",
      "5320\n",
      "5340\n",
      "5360\n",
      "5360\n",
      "5380\n",
      "5380\n",
      "5400\n",
      "5420\n",
      "5440\n",
      "5460\n",
      "5480\n",
      "5500\n",
      "5520\n",
      "5540\n",
      "5560\n",
      "5560\n",
      "5580\n",
      "5580\n",
      "5600\n",
      "5600\n",
      "5600\n",
      "5620\n",
      "5620\n",
      "5620\n",
      "5640\n",
      "5660\n",
      "5660\n",
      "5680\n",
      "5700\n",
      "5720\n",
      "5740\n",
      "5740\n",
      "5760\n",
      "5760\n",
      "5780\n",
      "5800\n",
      "5820\n",
      "5820\n",
      "5820\n",
      "5840\n",
      "5860\n",
      "5880\n",
      "5880\n",
      "5900\n",
      "5920\n",
      "5920\n",
      "5920\n",
      "5940\n",
      "5960\n",
      "5980\n",
      "5980\n",
      "5980\n",
      "6000\n",
      "6020\n",
      "6040\n",
      "6060\n",
      "6080\n",
      "6100\n",
      "6100\n",
      "6100\n",
      "6120\n",
      "6120\n",
      "6140\n",
      "6140\n",
      "6140\n",
      "6160\n",
      "6160\n",
      "6180\n",
      "6200\n",
      "6220\n",
      "6220\n",
      "6220\n",
      "6240\n",
      "6260\n",
      "6280\n",
      "6300\n",
      "6320\n",
      "6320\n",
      "6340\n",
      "6360\n",
      "6380\n",
      "6400\n",
      "6400\n",
      "6420\n",
      "6440\n",
      "6440\n",
      "6460\n",
      "6460\n",
      "6480\n",
      "6480\n",
      "6500\n",
      "6520\n",
      "6540\n",
      "6560\n",
      "6580\n",
      "6580\n",
      "6600\n",
      "6620\n",
      "6620\n",
      "6640\n",
      "6640\n",
      "6640\n",
      "6660\n",
      "6660\n",
      "6680\n",
      "6700\n",
      "6720\n",
      "6740\n",
      "6740\n",
      "6740\n",
      "6760\n",
      "6780\n",
      "6780\n",
      "6800\n",
      "6820\n",
      "6840\n",
      "6840\n",
      "6860\n",
      "6860\n",
      "6860\n",
      "6880\n",
      "6880\n",
      "6880\n",
      "6900\n",
      "6900\n",
      "6920\n",
      "6940\n",
      "6960\n",
      "6960\n",
      "6980\n",
      "6980\n",
      "7000\n",
      "7020\n",
      "7020\n",
      "7040\n",
      "7040\n",
      "7060\n",
      "7080\n",
      "7100\n",
      "7120\n",
      "7120\n",
      "7120\n",
      "7140\n",
      "7160\n",
      "7180\n",
      "7180\n",
      "7200\n",
      "7220\n",
      "7220\n",
      "7240\n",
      "7260\n",
      "7260\n",
      "7260\n",
      "7280\n",
      "7300\n",
      "7320\n",
      "7340\n",
      "7360\n",
      "7360\n",
      "7380\n",
      "7400\n",
      "7420\n",
      "7420\n",
      "7420\n",
      "7440\n",
      "7460\n",
      "7460\n",
      "7480\n",
      "7480\n",
      "7500\n",
      "7500\n",
      "7520\n",
      "7540\n",
      "7540\n",
      "7560\n",
      "7560\n",
      "7560\n",
      "7580\n",
      "7580\n",
      "7600\n",
      "7600\n",
      "7600\n",
      "7620\n",
      "7620\n",
      "7640\n",
      "7660\n",
      "7660\n",
      "7680\n",
      "7680\n",
      "7680\n",
      "7700\n",
      "7700\n",
      "7720\n",
      "7720\n",
      "7740\n",
      "7760\n",
      "7760\n",
      "7780\n",
      "7780\n",
      "7780\n",
      "7800\n",
      "7800\n",
      "7800\n",
      "7820\n",
      "7840\n",
      "7860\n",
      "7880\n",
      "7900\n",
      "7920\n",
      "7940\n",
      "7940\n",
      "7940\n",
      "7960\n",
      "7980\n",
      "7980\n",
      "7980\n",
      "8000\n",
      "8000\n",
      "8000\n",
      "8020\n",
      "8040\n",
      "8040\n",
      "8040\n",
      "8060\n",
      "8060\n",
      "8080\n",
      "8080\n",
      "8100\n",
      "8100\n",
      "8120\n",
      "8120\n",
      "8140\n",
      "8160\n",
      "8180\n",
      "8180\n",
      "8200\n",
      "8200\n",
      "8220\n",
      "8240\n",
      "8260\n",
      "8260\n",
      "8260\n",
      "8280\n",
      "8280\n",
      "8300\n",
      "8320\n",
      "8340\n",
      "8360\n",
      "8360\n",
      "8380\n",
      "8400\n",
      "8400\n",
      "8420\n",
      "8420\n",
      "8440\n",
      "8460\n",
      "8460\n",
      "8480\n",
      "8500\n",
      "8520\n",
      "8520\n",
      "8520\n",
      "8540\n",
      "8560\n",
      "8580\n",
      "8600\n",
      "8600\n",
      "8600\n",
      "8620\n",
      "8620\n",
      "8620\n",
      "8640\n",
      "8640\n",
      "8640\n",
      "8660\n",
      "8660\n",
      "8680\n",
      "8700\n",
      "8700\n",
      "8720\n",
      "8740\n",
      "8740\n",
      "8740\n",
      "8760\n",
      "8780\n",
      "8800\n",
      "8820\n",
      "8820\n",
      "8840\n",
      "8840\n",
      "8840\n",
      "8860\n",
      "8880\n",
      "8900\n",
      "8900\n",
      "8900\n",
      "8920\n",
      "8940\n",
      "8940\n",
      "8960\n",
      "8980\n",
      "8980\n",
      "9000\n",
      "9000\n",
      "9000\n",
      "9020\n",
      "9020\n",
      "9040\n",
      "9060\n",
      "9060\n",
      "9080\n",
      "9080\n",
      "9100\n",
      "9100\n",
      "9120\n",
      "9120\n",
      "9120\n",
      "9140\n",
      "9160\n",
      "9180\n",
      "9180\n",
      "9200\n",
      "9220\n",
      "9240\n",
      "9260\n",
      "9280\n",
      "9300\n",
      "9300\n",
      "9320\n",
      "9340\n",
      "9340\n",
      "9360\n",
      "9360\n",
      "9380\n",
      "9400\n",
      "9400\n",
      "9400\n",
      "9420\n",
      "9420\n",
      "9440\n",
      "9460\n",
      "9480\n",
      "9480\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9520\n",
      "9520\n",
      "9540\n",
      "9560\n",
      "9580\n",
      "9580\n",
      "9600\n",
      "9600\n",
      "9600\n",
      "9620\n",
      "9640\n",
      "9640\n",
      "9640\n",
      "9660\n",
      "9680\n",
      "9700\n",
      "9720\n",
      "9720\n",
      "9720\n",
      "9740\n",
      "9760\n",
      "9780\n",
      "9780\n",
      "9800\n",
      "9800\n",
      "9800\n",
      "9820\n",
      "9840\n",
      "9860\n",
      "9860\n",
      "9880\n",
      "9880\n",
      "9880\n",
      "9900\n",
      "9920\n",
      "9920\n",
      "9940\n",
      "9960\n",
      "9980\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10020\n",
      "10020\n",
      "10020\n",
      "10040\n",
      "10060\n",
      "10060\n",
      "10080\n",
      "10080\n",
      "10100\n",
      "10120\n",
      "10140\n",
      "10160\n",
      "10160\n",
      "10180\n",
      "10200\n",
      "10220\n",
      "10220\n",
      "10240\n",
      "10260\n",
      "10280\n",
      "10300\n",
      "10320\n",
      "10340\n",
      "10360\n",
      "10380\n",
      "10400\n",
      "10400\n",
      "10420\n",
      "10420\n",
      "10440\n",
      "10460\n",
      "10480\n",
      "10480\n",
      "10480\n",
      "10500\n",
      "10500\n",
      "10500\n",
      "10520\n",
      "10540\n",
      "10560\n",
      "10560\n",
      "10560\n",
      "10580\n",
      "10600\n",
      "10600\n",
      "10620\n",
      "10620\n",
      "10620\n",
      "10640\n",
      "10660\n",
      "10660\n",
      "10660\n",
      "10680\n",
      "10700\n",
      "10700\n",
      "10700\n",
      "10720\n",
      "10740\n",
      "10740\n",
      "10740\n",
      "10760\n",
      "10780\n",
      "10780\n",
      "10780\n",
      "10800\n",
      "10800\n",
      "10820\n",
      "10820\n",
      "10820\n",
      "10840\n",
      "10840\n",
      "10840\n",
      "10860\n",
      "10880\n",
      "10880\n",
      "10900\n",
      "10900\n",
      "10920\n",
      "10940\n",
      "10940\n",
      "10940\n",
      "10960\n",
      "10980\n",
      "10980\n",
      "11000\n",
      "11000\n",
      "11020\n",
      "11040\n",
      "11040\n",
      "11040\n",
      "11060\n",
      "11060\n",
      "11080\n",
      "11080\n",
      "11100\n",
      "11120\n",
      "11140\n",
      "11140\n",
      "11160\n",
      "11160\n",
      "11160\n",
      "11180\n",
      "11180\n",
      "11200\n",
      "11220\n",
      "11240\n",
      "11240\n",
      "11260\n",
      "11280\n",
      "11280\n",
      "11300\n",
      "11320\n",
      "11340\n",
      "11340\n",
      "11360\n",
      "11380\n",
      "11400\n",
      "11400\n",
      "11420\n",
      "11420\n",
      "11420\n",
      "11460\n",
      "11460\n",
      "11500\n",
      "11500\n",
      "11520\n",
      "11520\n",
      "11540\n",
      "11560\n",
      "11560\n",
      "11580\n",
      "11600\n",
      "11620\n",
      "11640\n",
      "11660\n",
      "11680\n",
      "11680\n",
      "11700\n",
      "11720\n",
      "11740\n",
      "11780\n",
      "11800\n",
      "11820\n",
      "11820\n",
      "11840\n",
      "11860\n",
      "11880\n",
      "11900\n",
      "11920\n",
      "11940\n",
      "11960\n",
      "11980\n",
      "11980\n",
      "12000\n",
      "12020\n",
      "12040\n",
      "12060\n",
      "12080\n",
      "12100\n",
      "12120\n",
      "12120\n",
      "12120\n",
      "12140\n",
      "12140\n",
      "12160\n",
      "12180\n",
      "12180\n",
      "12200\n",
      "12220\n",
      "12240\n",
      "12240\n",
      "12260\n",
      "12280\n",
      "12300\n",
      "12320\n",
      "12320\n",
      "12340\n",
      "12360\n",
      "12380\n",
      "12400\n",
      "12420\n",
      "12440\n",
      "12460\n",
      "12480\n",
      "12500\n",
      "12500\n",
      "12520\n",
      "12520\n",
      "12540\n",
      "12540\n",
      "12560\n",
      "12580\n",
      "12600\n",
      "12620\n",
      "12640\n",
      "12640\n",
      "12660\n",
      "12660\n",
      "12680\n",
      "12700\n",
      "12700\n",
      "12720\n",
      "12720\n",
      "12740\n",
      "12760\n",
      "12760\n",
      "12760\n",
      "12780\n",
      "12800\n",
      "12820\n",
      "12840\n",
      "12840\n",
      "12840\n",
      "12860\n",
      "12860\n",
      "12880\n",
      "12900\n",
      "12900\n",
      "12900\n",
      "12920\n",
      "12940\n",
      "12960\n",
      "12960\n",
      "12960\n",
      "12980\n",
      "12980\n",
      "13000\n",
      "13000\n",
      "13020\n",
      "13020\n",
      "13040\n",
      "13040\n",
      "13060\n",
      "13060\n",
      "13060\n",
      "13080\n",
      "13080\n",
      "13100\n",
      "13100\n",
      "13120\n",
      "13140\n",
      "13160\n",
      "13160\n",
      "13180\n",
      "13180\n",
      "13180\n",
      "13200\n",
      "13220\n",
      "13220\n",
      "13240\n",
      "13260\n",
      "13280\n",
      "13300\n",
      "13320\n",
      "13340\n",
      "13340\n",
      "13340\n",
      "13360\n",
      "13380\n",
      "13400\n",
      "13400\n",
      "13420\n",
      "13440\n",
      "13460\n",
      "13460\n",
      "13480\n",
      "13500\n",
      "13500\n",
      "13520\n",
      "13540\n",
      "13560\n",
      "13580\n",
      "13600\n",
      "13600\n",
      "13620\n",
      "13640\n",
      "13660\n",
      "13660\n",
      "13680\n",
      "13700\n",
      "13700\n",
      "13720\n",
      "13740\n",
      "13760\n",
      "13760\n",
      "13760\n",
      "13780\n",
      "13780\n",
      "13800\n",
      "13800\n",
      "13820\n",
      "13840\n",
      "13860\n",
      "13880\n",
      "13880\n",
      "13880\n",
      "13900\n",
      "13920\n",
      "13940\n",
      "13940\n",
      "13960\n",
      "13960\n",
      "13960\n",
      "13980\n",
      "14000\n",
      "14000\n",
      "14000\n",
      "14020\n",
      "14020\n",
      "14040\n",
      "14060\n",
      "14080\n",
      "14100\n",
      "14120\n",
      "14140\n",
      "14140\n",
      "14160\n",
      "14160\n",
      "14180\n",
      "14200\n",
      "14200\n",
      "14200\n",
      "14220\n",
      "14220\n",
      "14240\n",
      "14260\n",
      "14260\n",
      "14260\n",
      "14280\n",
      "14280\n",
      "14280\n",
      "14300\n",
      "14320\n",
      "14320\n",
      "14340\n",
      "14360\n",
      "14380\n",
      "14380\n",
      "14380\n",
      "14400\n",
      "14400\n",
      "14400\n",
      "14420\n",
      "14440\n",
      "14460\n",
      "14480\n",
      "14500\n",
      "14500\n",
      "14520\n",
      "14540\n",
      "14560\n",
      "14580\n",
      "14580\n",
      "14600\n",
      "14600\n",
      "14620\n",
      "14640\n",
      "14660\n",
      "14660\n",
      "14660\n",
      "14680\n",
      "14680\n",
      "14680\n",
      "14700\n",
      "14720\n",
      "14740\n",
      "14760\n",
      "14760\n",
      "14780\n",
      "14780\n",
      "14800\n",
      "14820\n",
      "14840\n",
      "14860\n",
      "14860\n",
      "14860\n",
      "14880\n",
      "14900\n",
      "14920\n",
      "14940\n",
      "14960\n",
      "14960\n",
      "14980\n",
      "14980\n",
      "14980\n",
      "15000\n",
      "15020\n",
      "15040\n",
      "15060\n",
      "15060\n",
      "15060\n",
      "15080\n",
      "15100\n",
      "15100\n",
      "15120\n",
      "15140\n",
      "15140\n",
      "15140\n",
      "15160\n",
      "15180\n",
      "15200\n",
      "15200\n",
      "15220\n",
      "15240\n",
      "15260\n",
      "15260\n",
      "15260\n",
      "15280\n",
      "15280\n",
      "15300\n",
      "15320\n",
      "15320\n",
      "15320\n",
      "15340\n",
      "15360\n",
      "15360\n",
      "15360\n",
      "15380\n",
      "15400\n",
      "15420\n",
      "15440\n",
      "15460\n",
      "15480\n",
      "15480\n",
      "15480\n",
      "15500\n",
      "15500\n",
      "15520\n",
      "15520\n",
      "15520\n",
      "15540\n",
      "15540\n",
      "15540\n",
      "15560\n",
      "15580\n",
      "15600\n",
      "15620\n",
      "15620\n",
      "15640\n",
      "15660\n",
      "15660\n",
      "15680\n",
      "15700\n",
      "15720\n",
      "15720\n",
      "15720\n",
      "15740\n",
      "15740\n",
      "15760\n",
      "15780\n",
      "15780\n",
      "15780\n",
      "15800\n",
      "15820\n",
      "15820\n",
      "15840\n",
      "15860\n",
      "15860\n",
      "15880\n",
      "15900\n",
      "15920\n",
      "15940\n",
      "15960\n",
      "15980\n",
      "15980\n",
      "16000\n",
      "16020\n",
      "16020\n",
      "16020\n",
      "16040\n",
      "16060\n",
      "16060\n",
      "16080\n",
      "16080\n",
      "16100\n",
      "16120\n",
      "16140\n",
      "16140\n",
      "16160\n",
      "16160\n",
      "16160\n",
      "16180\n",
      "16200\n",
      "16200\n",
      "16220\n",
      "16240\n",
      "16240\n",
      "16240\n",
      "16260\n",
      "16280\n",
      "16280\n",
      "16300\n",
      "16320\n",
      "16340\n",
      "16360\n",
      "16380\n",
      "16400\n",
      "16420\n",
      "16420\n",
      "16440\n",
      "16460\n",
      "16460\n",
      "16480\n",
      "16500\n",
      "16520\n",
      "16540\n",
      "16560\n",
      "16560\n",
      "16580\n",
      "16580\n",
      "16580\n",
      "16600\n",
      "16600\n",
      "16620\n",
      "16640\n",
      "16660\n",
      "16660\n",
      "16680\n",
      "16700\n",
      "16700\n",
      "16700\n",
      "16720\n",
      "16720\n",
      "16740\n",
      "16760\n",
      "16780\n",
      "16780\n",
      "16800\n",
      "16800\n",
      "16800\n",
      "16820\n",
      "16820\n",
      "16820\n",
      "16840\n",
      "16860\n",
      "16860\n",
      "16860\n",
      "16880\n",
      "16880\n",
      "16900\n",
      "16900\n",
      "16920\n",
      "16940\n",
      "16960\n",
      "16980\n",
      "17000\n",
      "17020\n",
      "17040\n",
      "17060\n",
      "17080\n",
      "17080\n",
      "17080\n",
      "17100\n",
      "17120\n",
      "17140\n",
      "17160\n",
      "17160\n",
      "17160\n",
      "17180\n",
      "17180\n",
      "17180\n",
      "17200\n",
      "17200\n",
      "17200\n",
      "17220\n",
      "17240\n",
      "17260\n",
      "17260\n",
      "17280\n",
      "17300\n",
      "17320\n",
      "17340\n",
      "17340\n",
      "17340\n",
      "17360\n",
      "17380\n",
      "17400\n",
      "17420\n",
      "17440\n",
      "17440\n",
      "17440\n",
      "17460\n",
      "17480\n",
      "17500\n",
      "17520\n",
      "17540\n",
      "17560\n",
      "17560\n",
      "17580\n",
      "17600\n",
      "17600\n",
      "17600\n",
      "17620\n",
      "17640\n",
      "17660\n",
      "17680\n",
      "17680\n",
      "17700\n",
      "17720\n",
      "17740\n",
      "17740\n",
      "17760\n",
      "17760\n",
      "17760\n",
      "17780\n",
      "17780\n",
      "17780\n",
      "17800\n",
      "17820\n",
      "17840\n",
      "17840\n",
      "17840\n",
      "17860\n",
      "17860\n",
      "17860\n",
      "17880\n",
      "17900\n",
      "17900\n",
      "17920\n",
      "17920\n",
      "17940\n",
      "17940\n",
      "17940\n",
      "17960\n",
      "17980\n",
      "18000\n",
      "18020\n",
      "18020\n",
      "18020\n",
      "18040\n",
      "18060\n",
      "18080\n",
      "18080\n",
      "18080\n",
      "18100\n",
      "18120\n",
      "18120\n",
      "18140\n",
      "18160\n",
      "18160\n",
      "18180\n",
      "18200\n",
      "18200\n",
      "18200\n",
      "18220\n",
      "18240\n",
      "18260\n",
      "18280\n",
      "18280\n",
      "18300\n",
      "18300\n",
      "18300\n",
      "18320\n",
      "18340\n",
      "18360\n",
      "18360\n",
      "18380\n",
      "18400\n",
      "18400\n",
      "18400\n",
      "18420\n",
      "18440\n",
      "18460\n",
      "18480\n",
      "18480\n",
      "18500\n",
      "18520\n",
      "18520\n",
      "18540\n",
      "18540\n",
      "18540\n",
      "18560\n",
      "18580\n",
      "18600\n",
      "18600\n",
      "18600\n",
      "18620\n",
      "18640\n",
      "18640\n",
      "18660\n",
      "18680\n",
      "18700\n",
      "18700\n",
      "18720\n",
      "18720\n",
      "18740\n",
      "18760\n",
      "18780\n",
      "18780\n",
      "18800\n",
      "18800\n",
      "18820\n",
      "18840\n",
      "18840\n",
      "18860\n",
      "18860\n",
      "18880\n",
      "18880\n",
      "18900\n",
      "18920\n",
      "18920\n",
      "18940\n",
      "18940\n",
      "18940\n",
      "18960\n",
      "18980\n",
      "18980\n",
      "19000\n",
      "19020\n",
      "19040\n",
      "19040\n",
      "19060\n",
      "19060\n",
      "19080\n",
      "19080\n",
      "19080\n",
      "19100\n",
      "19120\n",
      "19140\n",
      "19160\n",
      "19160\n",
      "19180\n",
      "19180\n",
      "19200\n",
      "19220\n",
      "19240\n",
      "19260\n",
      "19260\n",
      "19260\n",
      "19280\n",
      "19300\n",
      "19320\n",
      "19320\n",
      "19340\n",
      "19340\n",
      "19360\n",
      "19380\n",
      "19380\n",
      "19380\n",
      "19400\n",
      "19420\n",
      "19440\n",
      "19460\n",
      "19480\n",
      "19480\n",
      "19500\n",
      "19500\n",
      "19520\n",
      "19540\n",
      "19560\n",
      "19580\n",
      "19580\n",
      "19580\n",
      "19600\n",
      "19600\n",
      "19600\n",
      "19620\n",
      "19620\n",
      "19620\n",
      "19640\n",
      "19660\n",
      "19660\n",
      "19660\n",
      "19680\n",
      "19700\n",
      "19700\n",
      "19720\n",
      "19740\n",
      "19760\n",
      "19760\n",
      "19760\n",
      "19780\n",
      "19780\n",
      "19800\n",
      "19820\n",
      "19840\n",
      "19860\n",
      "19880\n",
      "19900\n",
      "19900\n",
      "19900\n",
      "19920\n",
      "19940\n",
      "19940\n",
      "19940\n",
      "19960\n",
      "19980\n",
      "19980\n",
      "19980\n",
      "20000\n",
      "20000\n",
      "20020\n",
      "20020\n",
      "20040\n",
      "20040\n",
      "20060\n",
      "20080\n",
      "20100\n",
      "20120\n",
      "20140\n",
      "20160\n",
      "20160\n",
      "20180\n",
      "20200\n",
      "20220\n",
      "20220\n",
      "20240\n",
      "20240\n",
      "20240\n",
      "20260\n",
      "20280\n",
      "20280\n",
      "20280\n",
      "20300\n",
      "20320\n",
      "20340\n",
      "20340\n",
      "20360\n",
      "20380\n",
      "20400\n",
      "20400\n",
      "20420\n",
      "20440\n",
      "20440\n",
      "20460\n",
      "20480\n",
      "20500\n",
      "20500\n",
      "20520\n",
      "20540\n",
      "20540\n",
      "20540\n",
      "20560\n",
      "20580\n",
      "20580\n",
      "20580\n",
      "20600\n",
      "20620\n",
      "20640\n",
      "20660\n",
      "20660\n",
      "20680\n",
      "20700\n",
      "20720\n",
      "20740\n",
      "20760\n",
      "20760\n",
      "20760\n",
      "20780\n",
      "20800\n",
      "20820\n",
      "20840\n",
      "20840\n",
      "20860\n",
      "20860\n",
      "20880\n",
      "20900\n",
      "20920\n",
      "20940\n",
      "20960\n",
      "20980\n",
      "20980\n",
      "21000\n",
      "21000\n",
      "21020\n",
      "21040\n",
      "21060\n",
      "21080\n",
      "21080\n",
      "21100\n",
      "21100\n",
      "21100\n",
      "21120\n",
      "21120\n",
      "21140\n",
      "21160\n",
      "21160\n",
      "21180\n",
      "21180\n",
      "21180\n",
      "21200\n",
      "21200\n",
      "21200\n",
      "21220\n",
      "21240\n",
      "21260\n",
      "21260\n",
      "21280\n",
      "21300\n",
      "21300\n",
      "21300\n",
      "21320\n",
      "21320\n",
      "21320\n",
      "21340\n",
      "21340\n",
      "21340\n",
      "21360\n",
      "21380\n",
      "21380\n",
      "21400\n",
      "21420\n",
      "21420\n",
      "21440\n",
      "21440\n",
      "21460\n",
      "21460\n",
      "21480\n",
      "21480\n",
      "21500\n",
      "21500\n",
      "21520\n",
      "21520\n",
      "21540\n",
      "21560\n",
      "21580\n",
      "21600\n",
      "21600\n",
      "21620\n",
      "21640\n",
      "21660\n",
      "21660\n",
      "21680\n",
      "21700\n",
      "21720\n",
      "21740\n",
      "21740\n",
      "21760\n",
      "21780\n",
      "21780\n",
      "21800\n",
      "21800\n",
      "21820\n",
      "21840\n",
      "21840\n",
      "21860\n",
      "21880\n",
      "21880\n",
      "21900\n",
      "21920\n",
      "21920\n",
      "21940\n",
      "21960\n",
      "21960\n",
      "21960\n",
      "21980\n",
      "21980\n",
      "21980\n",
      "22000\n",
      "22000\n",
      "22020\n",
      "22040\n",
      "22060\n",
      "22060\n",
      "22080\n",
      "22080\n",
      "22080\n",
      "22100\n",
      "22120\n",
      "22140\n",
      "22140\n",
      "22140\n",
      "22160\n",
      "22160\n",
      "22160\n",
      "22180\n",
      "22200\n",
      "22200\n",
      "22200\n",
      "22220\n",
      "22220\n",
      "22240\n",
      "22240\n",
      "22240\n",
      "22260\n",
      "22280\n",
      "22300\n",
      "22320\n",
      "22320\n",
      "22320\n",
      "22340\n",
      "22360\n",
      "22380\n",
      "22380\n",
      "22400\n",
      "22420\n",
      "22440\n",
      "22440\n",
      "22460\n",
      "22480\n",
      "22480\n",
      "22480\n",
      "22500\n",
      "22520\n",
      "22540\n",
      "22560\n",
      "22560\n",
      "22580\n",
      "22600\n",
      "22600\n",
      "22620\n",
      "22620\n",
      "22640\n",
      "22660\n",
      "22680\n",
      "22680\n",
      "22700\n",
      "22720\n",
      "22720\n",
      "22740\n",
      "22740\n",
      "22760\n",
      "22760\n",
      "22760\n",
      "22780\n",
      "22800\n",
      "22800\n",
      "22820\n",
      "22840\n",
      "22860\n",
      "22880\n",
      "22880\n",
      "22900\n",
      "22900\n",
      "22900\n",
      "22920\n",
      "22920\n",
      "22940\n",
      "22960\n",
      "22960\n",
      "22980\n",
      "22980\n",
      "23000\n",
      "23020\n",
      "23040\n",
      "23060\n",
      "23060\n",
      "23080\n",
      "23100\n",
      "23120\n",
      "23120\n",
      "23120\n",
      "23140\n",
      "23140\n",
      "23140\n",
      "23160\n",
      "23180\n",
      "23180\n",
      "23180\n",
      "23200\n",
      "23220\n",
      "23220\n",
      "23240\n",
      "23240\n",
      "23260\n",
      "23280\n",
      "23280\n",
      "23300\n",
      "23320\n",
      "23320\n",
      "23340\n",
      "23340\n",
      "23340\n",
      "23360\n",
      "23360\n",
      "23360\n",
      "23380\n",
      "23380\n",
      "23380\n",
      "23400\n",
      "23400\n",
      "23420\n",
      "23440\n",
      "23460\n",
      "23480\n",
      "23500\n",
      "23520\n",
      "23520\n"
     ]
    }
   ],
   "source": [
    "all_genres=[]\n",
    "\n",
    "big_data.genre1=big_data.genre1.str.strip()\n",
    "big_data.genre2=big_data.genre2.str.strip()\n",
    "big_data.genre3=big_data.genre3.str.strip()\n",
    "\n",
    "\n",
    "big_data.genre1.apply(lambda x: all_genres.append(x))\n",
    "big_data.genre2.apply(lambda x: all_genres.append(x))\n",
    "big_data.genre3.apply(lambda x: all_genres.append(x))\n",
    "\n",
    "total_genres = set(all_genres)\n",
    "\n",
    "genre_dict_list=[] \n",
    "print 'here'\n",
    "for row in range(len(big_data)): \n",
    "    genre_dict_list.append({x:0 for x in total_genres})\n",
    "print 'blah'        \n",
    "for row in range(len(big_data)):\n",
    "    \n",
    "    for genre in genre_dict_list[row]:\n",
    "        if (big_data.iloc[row][9] == genre) or (big_data.iloc[row][10] == genre) or (big_data.iloc[row][11] == genre):\n",
    "            genre_dict_list[row][genre] = 1 \n",
    "            if row % 20 == 0:\n",
    "                print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_df = pd.DataFrame.from_records(genre_dict_list)\n",
    "big_feature_with_genre_df = pd.concat([big_feature_df, genre_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_feature_df=big_feature_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name svr",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-616-8b81f9612ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name svr"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, linear_model, datasets\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-615-761df0ba4a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_feature_with_genre_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_target_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svr' is not defined"
     ]
    }
   ],
   "source": [
    "meh = svr()\n",
    "meh.fit(big_feature_with_genre_df, big_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
